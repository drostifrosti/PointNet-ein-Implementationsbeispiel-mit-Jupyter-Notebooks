{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Inhalt](./Einleitung.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PointNet-Klassifizierungsnetzwerk\n",
    "\n",
    "Im Rahmen dieses Notebooks wird anhand einer beispielhaften Implementation die praktische Anwendung der PointNet-Architektur zur Klassifizierung dreidimensionaler Objekte in Form von Punktwolken gezeigt.\n",
    "\n",
    "## Anwendungshinweis\n",
    "\n",
    "Da es sich bei dem Code in diesem Notebook um ein zusammenhängendes Script handelt, empfiehlt sich bei Änderungen stets das Zurücksetzen des Kernels. Hierfür wird unter dem Reiter \"Kernel\" die Option \"Restart & Run all\" ausgewählt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architektur\n",
    "\n",
    "![workflow](\"./images/PointNet-Klassifikation-Marked.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Für die hier analyiserte Keras-Implementation der PointNet-Architektur werden drei Pakete benötigt:\n",
    "* NumPy für die Verwaltung von Datenstrukturen sowie der Handhabung von Vektoren und Matrizen\n",
    "* H5Py als Schnittstelle für das HDF5-Datenformat\n",
    "* Tensorflow für die Verwendung der Keras-API, welche Teil des Tensorflow-Cores ist\n",
    "\n",
    "Darüber hinaus werden die nötigen Keras-Pakete für die Schichttypen und die erzeugung des Modells importiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Reshape, Dropout\n",
    "from keras.layers import Convolution1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Lambda\n",
    "from keras.utils import np_utils\n",
    "import h5py\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_mul(A, B):\n",
    "    return tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_point_cloud(batch_data):\n",
    "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "        rotation is per shape based along up direction\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
    "    \"\"\" Randomly jitter points. jittering is per point.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, jittered batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    assert(clip > 0)\n",
    "    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1 * clip, clip)\n",
    "    jittered_data += batch_data\n",
    "    return jittered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of points in each sample\n",
    "num_points = 2048\n",
    "\n",
    "# number of categories\n",
    "k = 40\n",
    "\n",
    "# define optimizer\n",
    "adam = optimizers.Adam(lr=0.001, decay=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------ Pointnet Architecture\n",
    "# input_Transformation_net\n",
    "input_points = Input(shape=(num_points, 3))\n",
    "x = Convolution1D(64, 1, activation='relu',\n",
    "                  input_shape=(num_points, 3))(input_points)\n",
    "x = BatchNormalization()(x)\n",
    "x = Convolution1D(128, 1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Convolution1D(1024, 1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=num_points)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\n",
    "input_T = Reshape((3, 3))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward net\n",
    "g = Lambda(mat_mul, arguments={'B': input_T})(input_points)\n",
    "g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
    "g = BatchNormalization()(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature transform net\n",
    "f = Convolution1D(64, 1, activation='relu')(g)\n",
    "f = BatchNormalization()(f)\n",
    "f = Convolution1D(128, 1, activation='relu')(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Convolution1D(1024, 1, activation='relu')(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = MaxPooling1D(pool_size=num_points)(f)\n",
    "f = Dense(512, activation='relu')(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\n",
    "feature_T = Reshape((64, 64))(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward net\n",
    "g = Lambda(mat_mul, arguments={'B': feature_T})(g)\n",
    "g = Convolution1D(64, 1, activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = Convolution1D(128, 1, activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "g = Convolution1D(1024, 1, activation='relu')(g)\n",
    "g = BatchNormalization()(g)\n",
    "\n",
    "# global_feature\n",
    "global_feature = MaxPooling1D(pool_size=num_points)(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point_net_cls\n",
    "#TODO: Dropout auf 0.3\n",
    "c = Dense(512, activation='relu')(global_feature)\n",
    "c = BatchNormalization()(c)\n",
    "c = Dropout(rate=0.3)(c)\n",
    "c = Dense(256, activation='relu')(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Dropout(rate=0.3)(c)\n",
    "c = Dense(k, activation='softmax')(c)\n",
    "prediction = Flatten()(c)\n",
    "# --------------------------------------------------end of pointnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model summary\n",
    "model = Model(inputs=input_points, outputs=prediction)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train points and labels\n",
    "path = os.path.dirname(os.path.abspath(''))\n",
    "train_path = os.path.join(path, \"PrepData\")\n",
    "filenames = [d for d in os.listdir(train_path)]\n",
    "print(train_path)\n",
    "print(filenames)\n",
    "train_points = None\n",
    "train_labels = None\n",
    "for d in filenames:\n",
    "    cur_points, cur_labels = load_h5(os.path.join(train_path, d))\n",
    "    cur_points = cur_points.reshape(1, -1, 3)\n",
    "    cur_labels = cur_labels.reshape(1, -1)\n",
    "    if train_labels is None or train_points is None:\n",
    "        train_labels = cur_labels\n",
    "        train_points = cur_points\n",
    "    else:\n",
    "        train_labels = np.hstack((train_labels, cur_labels))\n",
    "        train_points = np.hstack((train_points, cur_points))\n",
    "train_points_r = train_points.reshape(-1, num_points, 3)\n",
    "train_labels_r = train_labels.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test points and labels\n",
    "test_path = os.path.join(path, \"PrepData_test\")\n",
    "filenames = [d for d in os.listdir(test_path)]\n",
    "print(test_path)\n",
    "print(filenames)\n",
    "test_points = None\n",
    "test_labels = None\n",
    "for d in filenames:\n",
    "    cur_points, cur_labels = load_h5(os.path.join(test_path, d))\n",
    "    cur_points = cur_points.reshape(1, -1, 3)\n",
    "    cur_labels = cur_labels.reshape(1, -1)\n",
    "    if test_labels is None or test_points is None:\n",
    "        test_labels = cur_labels\n",
    "        test_points = cur_points\n",
    "    else:\n",
    "        test_labels = np.hstack((test_labels, cur_labels))\n",
    "        test_points = np.hstack((test_points, cur_points))\n",
    "test_points_r = test_points.reshape(-1, num_points, 3)\n",
    "test_labels_r = test_labels.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to categorical\n",
    "Y_train = np_utils.to_categorical(train_labels_r, k)\n",
    "Y_test = np_utils.to_categorical(test_labels_r, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data\n",
    "for i in range(1,50):\n",
    "    #model.fit(train_points_r, Y_train, batch_size=32, epochs=1, shuffle=True, verbose=1)\n",
    "    # rotate and jitter the points\n",
    "    train_points_rotate = rotate_point_cloud(train_points_r)\n",
    "    train_points_jitter = jitter_point_cloud(train_points_rotate)\n",
    "    model.fit(train_points_jitter, Y_train, batch_size=32, epochs=1, shuffle=True, verbose=1)\n",
    "    s = \"Current epoch is:\" + str(i)\n",
    "    print(s)\n",
    "    if i % 5 == 0:\n",
    "        score = model.evaluate(test_points_r, Y_test, verbose=1)\n",
    "        print('Test loss: ', score[0])\n",
    "        print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "score = model.evaluate(test_points_r, Y_test, verbose=1)\n",
    "print('Test loss: ', score[0])\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
